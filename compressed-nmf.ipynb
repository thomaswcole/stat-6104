{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_compression(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using a randomized algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Draw a Gaussian random matrix Omega_L\n",
    "    Omega_L = np.random.randn(n, d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression matrix Q shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(100, 50)  # Input matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Q = randomized_compression(A, r, rOV, w)\n",
    "print(\"Compression matrix Q shape:\", Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1 SRHT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "\n",
    "def randomized_compression_srht(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using SRHT as the test matrix.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure n is a power of 2 by padding A with zeros if necessary\n",
    "    n_padded = 2 ** int(np.ceil(np.log2(n)))  # Smallest power of 2 >= n\n",
    "    if n_padded != n:\n",
    "        A_padded = np.zeros((m, n_padded))\n",
    "        A_padded[:, :n] = A  # Pad with zeros\n",
    "        A = A_padded\n",
    "        n = n_padded\n",
    "\n",
    "    # Step 1: Generate SRHT matrix\n",
    "    H = hadamard(n)  # Hadamard matrix of size n x n\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (H @ D)[:, S] / np.sqrt(d)  # SRHT matrix (n x d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: (1000, 60)\n",
      "Orthogonality check (Q^T Q):\n",
      "[[ 1.  0.  0. ...  0.  0. -0.]\n",
      " [ 0.  1. -0. ... -0.  0.  0.]\n",
      " [ 0. -0.  1. ... -0. -0.  0.]\n",
      " ...\n",
      " [ 0. -0. -0. ...  1.  0.  0.]\n",
      " [ 0.  0. -0. ...  0.  1.  0.]\n",
      " [-0.  0.  0. ...  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of the input matrix A\n",
    "m = 1000  # Number of rows\n",
    "n = 500   # Number of columns\n",
    "\n",
    "# Generate a random matrix A ∈ R^(m x n)\n",
    "A = np.random.randn(m, n)\n",
    "\n",
    "# Set the parameters for the randomized compression\n",
    "r = 50    # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2     # Exponent for power iteration\n",
    "\n",
    "# Call the SRHT-based randomized compression function\n",
    "Q = randomized_compression_srht(A, r, rOV, w)\n",
    "\n",
    "# Print the shape of the resulting compression matrix Q\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected output: (1000, 60) since r + rOV = 50 + 10 = 60\n",
    "\n",
    "# Verify that Q is orthogonal (Q^T Q ≈ I)\n",
    "orthogonality_check = Q.T @ Q\n",
    "print(\"Orthogonality check (Q^T Q):\")\n",
    "print(np.round(orthogonality_check, 6))  # Should be close to the identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1 SRFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "def randomized_compression_srft(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using SRFT as the test matrix.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Generate SRFT matrix\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    F = fft(np.eye(n), axis=0)  # Fourier matrix of size n x n\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (F @ D)[S, :] / np.sqrt(d)  # SRFT matrix\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Random Compression Fig 2 (Multiplicative Updates)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_with_compression(A, r, rOV, w, max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Compute NMF with compression matrices L and R using multiplicative updates.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    X_k (numpy array): Nonnegative matrix (m x r)\n",
    "    Y_k (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression_srht(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression_srht(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Initialize Y_k with nonnegative values\n",
    "    Y_k = np.abs(np.random.randn(r, n))  # Y_k ∈ R^(r x n)\n",
    "\n",
    "    # Step 3: Compute compressed matrices\n",
    "    A_check = A @ R.T  # A_check ∈ R^(m x d)\n",
    "    A_hat = L.T @ A  # A_hat ∈ R^(d x n)\n",
    "\n",
    "    # Ensure compressed matrices are nonnegative\n",
    "    A_check = np.abs(A_check)\n",
    "    A_hat = np.abs(A_hat)\n",
    "\n",
    "    # Initialize X_k\n",
    "    X_k = np.abs(np.random.randn(m, r))  # X_k ∈ R^(m x r)\n",
    "\n",
    "    norm_A = np.linalg.norm(A, 'fro')\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    prev_error = np.inf\n",
    "    for k in range(max_iter):\n",
    "        # Step 4: Compute Y_check_k = Y_k R^T\n",
    "        Y_check_k = Y_k @ R.T  # Y_check_k ∈ R^(r x d)\n",
    "\n",
    "        # Step 5: Update X_k+1 using multiplicative updates\n",
    "        numerator_X = A_check @ Y_check_k.T\n",
    "        denominator_X = X_k @ (Y_check_k @ Y_check_k.T)\n",
    "        X_k_plus_1 = X_k * (numerator_X / denominator_X)\n",
    "\n",
    "        # Step 6: Compute X_hat_k+1 = L^T X_k+1\n",
    "        X_hat_k_plus_1 = L.T @ X_k_plus_1  # X_hat_k+1 ∈ R^(d x r)\n",
    "\n",
    "        # Step 7: Update Y_k+1 using multiplicative updates\n",
    "        numerator_Y = X_hat_k_plus_1.T @ A_hat\n",
    "        denominator_Y = (X_hat_k_plus_1.T @ X_hat_k_plus_1) @ Y_k\n",
    "        Y_k_plus_1 = Y_k * (numerator_Y / denominator_Y)\n",
    "\n",
    "        # Compute reconstruction error\n",
    "        reconstruction_error = np.linalg.norm(A - X_k_plus_1 @ Y_k_plus_1, 'fro')\n",
    "        normalized_error = reconstruction_error / norm_A  # Normalized error\n",
    "        print(f\"Iteration {k+1}: Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if abs(prev_error - normalized_error) < tol:\n",
    "            break\n",
    "        prev_error = normalized_error\n",
    "\n",
    "\n",
    "        # Update X_k and Y_k\n",
    "        X_k, Y_k = X_k_plus_1, Y_k_plus_1\n",
    "\n",
    "    return X_k, Y_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Normalized Reconstruction Error = 1.721853\n",
      "Iteration 2: Normalized Reconstruction Error = 1.721625\n",
      "Iteration 3: Normalized Reconstruction Error = 1.721383\n",
      "Iteration 4: Normalized Reconstruction Error = 1.721198\n",
      "Iteration 5: Normalized Reconstruction Error = 1.721072\n",
      "Iteration 6: Normalized Reconstruction Error = 1.721001\n",
      "Iteration 7: Normalized Reconstruction Error = 1.720982\n",
      "Iteration 8: Normalized Reconstruction Error = 1.721009\n",
      "Iteration 9: Normalized Reconstruction Error = 1.721076\n",
      "Iteration 10: Normalized Reconstruction Error = 1.721178\n",
      "Iteration 11: Normalized Reconstruction Error = 1.721309\n",
      "Iteration 12: Normalized Reconstruction Error = 1.721461\n",
      "Iteration 13: Normalized Reconstruction Error = 1.721631\n",
      "Iteration 14: Normalized Reconstruction Error = 1.721814\n",
      "Iteration 15: Normalized Reconstruction Error = 1.722006\n",
      "Iteration 16: Normalized Reconstruction Error = 1.722202\n",
      "Iteration 17: Normalized Reconstruction Error = 1.722402\n",
      "Iteration 18: Normalized Reconstruction Error = 1.722601\n",
      "Iteration 19: Normalized Reconstruction Error = 1.722800\n",
      "Iteration 20: Normalized Reconstruction Error = 1.722995\n",
      "Iteration 21: Normalized Reconstruction Error = 1.723187\n",
      "Iteration 22: Normalized Reconstruction Error = 1.723373\n",
      "Iteration 23: Normalized Reconstruction Error = 1.723555\n",
      "Iteration 24: Normalized Reconstruction Error = 1.723730\n",
      "Iteration 25: Normalized Reconstruction Error = 1.723900\n",
      "Iteration 26: Normalized Reconstruction Error = 1.724063\n",
      "Iteration 27: Normalized Reconstruction Error = 1.724220\n",
      "Iteration 28: Normalized Reconstruction Error = 1.724370\n",
      "Iteration 29: Normalized Reconstruction Error = 1.724513\n",
      "Iteration 30: Normalized Reconstruction Error = 1.724650\n",
      "Iteration 31: Normalized Reconstruction Error = 1.724780\n",
      "Iteration 32: Normalized Reconstruction Error = 1.724903\n",
      "Iteration 33: Normalized Reconstruction Error = 1.725018\n",
      "Iteration 34: Normalized Reconstruction Error = 1.725127\n",
      "Iteration 35: Normalized Reconstruction Error = 1.725228\n",
      "Iteration 36: Normalized Reconstruction Error = 1.725321\n",
      "Iteration 37: Normalized Reconstruction Error = 1.725408\n",
      "Iteration 38: Normalized Reconstruction Error = 1.725486\n",
      "Iteration 39: Normalized Reconstruction Error = 1.725558\n",
      "Iteration 40: Normalized Reconstruction Error = 1.725622\n",
      "Iteration 41: Normalized Reconstruction Error = 1.725680\n",
      "Iteration 42: Normalized Reconstruction Error = 1.725731\n",
      "Iteration 43: Normalized Reconstruction Error = 1.725775\n",
      "Iteration 44: Normalized Reconstruction Error = 1.725813\n",
      "Iteration 45: Normalized Reconstruction Error = 1.725846\n",
      "Iteration 46: Normalized Reconstruction Error = 1.725872\n",
      "Iteration 47: Normalized Reconstruction Error = 1.725894\n",
      "Iteration 48: Normalized Reconstruction Error = 1.725911\n",
      "Iteration 49: Normalized Reconstruction Error = 1.725923\n",
      "Iteration 50: Normalized Reconstruction Error = 1.725931\n",
      "Iteration 51: Normalized Reconstruction Error = 1.725935\n",
      "Iteration 52: Normalized Reconstruction Error = 1.725937\n",
      "Iteration 53: Normalized Reconstruction Error = 1.725936\n",
      "Iteration 54: Normalized Reconstruction Error = 1.725933\n",
      "Iteration 55: Normalized Reconstruction Error = 1.725928\n",
      "Iteration 56: Normalized Reconstruction Error = 1.725923\n",
      "Iteration 57: Normalized Reconstruction Error = 1.725919\n",
      "Iteration 58: Normalized Reconstruction Error = 1.725915\n",
      "Iteration 59: Normalized Reconstruction Error = 1.725912\n",
      "Iteration 60: Normalized Reconstruction Error = 1.725912\n",
      "Iteration 61: Normalized Reconstruction Error = 1.725914\n",
      "Iteration 62: Normalized Reconstruction Error = 1.725920\n",
      "Iteration 63: Normalized Reconstruction Error = 1.725930\n",
      "Iteration 64: Normalized Reconstruction Error = 1.725944\n",
      "Iteration 65: Normalized Reconstruction Error = 1.725964\n",
      "Iteration 66: Normalized Reconstruction Error = 1.725989\n",
      "Iteration 67: Normalized Reconstruction Error = 1.726019\n",
      "Iteration 68: Normalized Reconstruction Error = 1.726056\n",
      "Iteration 69: Normalized Reconstruction Error = 1.726098\n",
      "Iteration 70: Normalized Reconstruction Error = 1.726146\n",
      "Iteration 71: Normalized Reconstruction Error = 1.726200\n",
      "Iteration 72: Normalized Reconstruction Error = 1.726258\n",
      "Iteration 73: Normalized Reconstruction Error = 1.726321\n",
      "Iteration 74: Normalized Reconstruction Error = 1.726387\n",
      "Iteration 75: Normalized Reconstruction Error = 1.726456\n",
      "Iteration 76: Normalized Reconstruction Error = 1.726528\n",
      "Iteration 77: Normalized Reconstruction Error = 1.726600\n",
      "Iteration 78: Normalized Reconstruction Error = 1.726673\n",
      "Iteration 79: Normalized Reconstruction Error = 1.726747\n",
      "Iteration 80: Normalized Reconstruction Error = 1.726820\n",
      "Iteration 81: Normalized Reconstruction Error = 1.726892\n",
      "Iteration 82: Normalized Reconstruction Error = 1.726963\n",
      "Iteration 83: Normalized Reconstruction Error = 1.727032\n",
      "Iteration 84: Normalized Reconstruction Error = 1.727100\n",
      "Iteration 85: Normalized Reconstruction Error = 1.727167\n",
      "Iteration 86: Normalized Reconstruction Error = 1.727232\n",
      "Iteration 87: Normalized Reconstruction Error = 1.727295\n",
      "Iteration 88: Normalized Reconstruction Error = 1.727357\n",
      "Iteration 89: Normalized Reconstruction Error = 1.727419\n",
      "Iteration 90: Normalized Reconstruction Error = 1.727480\n",
      "Iteration 91: Normalized Reconstruction Error = 1.727541\n",
      "Iteration 92: Normalized Reconstruction Error = 1.727603\n",
      "Iteration 93: Normalized Reconstruction Error = 1.727666\n",
      "Iteration 94: Normalized Reconstruction Error = 1.727731\n",
      "Iteration 95: Normalized Reconstruction Error = 1.727798\n",
      "Iteration 96: Normalized Reconstruction Error = 1.727867\n",
      "Iteration 97: Normalized Reconstruction Error = 1.727940\n",
      "Iteration 98: Normalized Reconstruction Error = 1.728015\n",
      "Iteration 99: Normalized Reconstruction Error = 1.728095\n",
      "Iteration 100: Normalized Reconstruction Error = 1.728177\n",
      "Final X shape: (100, 5)\n",
      "Final Y shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Xk, Yk = nmf_with_compression(A, r, rOV, w)\n",
    "print(\"Final X shape:\", Xk.shape)\n",
    "print(\"Final Y shape:\", Yk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Compression (MU SkLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_mu(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.162617\n",
      "Final W shape: (100, 45)\n",
      "Final H shape: (45, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 45  # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_mu(A, r, rOV, w,1200,1e-5)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Reconstruction Error: 0.5412531442585214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tom\\Desktop\\python\\stat-6104\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition._nmf import _initialize_nmf\n",
    "\n",
    "def randomized_compression_srht(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a low-rank approximation Q of A using SRHT.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure n is a power of 2 by padding A with zeros if necessary\n",
    "    n_padded = 2 ** int(np.ceil(np.log2(n)))  # Smallest power of 2 >= n\n",
    "    if n_padded != n:\n",
    "        A_padded = np.zeros((m, n_padded))\n",
    "        A_padded[:, :n] = A  # Pad with zeros\n",
    "        A = A_padded\n",
    "        n = n_padded\n",
    "\n",
    "    # Step 1: Generate SRHT matrix\n",
    "    H = hadamard(n)  # Hadamard matrix of size n x n\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (H @ D)[:, S] / np.sqrt(d)  # SRHT matrix (n x d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def srht_nndsvd_initialization(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Combine SRHT and NNDSVD for NMF initialization.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Compute low-rank approximation using SRHT\n",
    "    Q = randomized_compression_srht(A, r, rOV, w)\n",
    "\n",
    "    # Step 2: Project Q onto the non-negative orthant\n",
    "    Q_non_neg = np.maximum(Q, 0)\n",
    "\n",
    "    # Step 3: Apply NNDSVD to Q_non_neg to initialize W and H\n",
    "    # Use the first r columns of Q_non_neg for initialization\n",
    "    W_init = Q_non_neg[:, :r]\n",
    "\n",
    "    # Initialize H using NNDSVD on the low-rank approximation\n",
    "    _, H_init = _initialize_nmf(A, r, init='nndsvd')\n",
    "\n",
    "    # Ensure W_init and H_init are C-contiguous\n",
    "    W_init = np.ascontiguousarray(W_init)\n",
    "    H_init = np.ascontiguousarray(H_init)\n",
    "\n",
    "    return W_init, H_init\n",
    "\n",
    "# Example usage\n",
    "m, n = 1000, 500  # Dimensions of A\n",
    "r = 50  # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2  # Power iteration exponent\n",
    "\n",
    "# Generate a random non-negative matrix A\n",
    "A = np.abs(np.random.randn(m, n))  # Ensure non-negativity\n",
    "\n",
    "# Initialize W and H using SRHT + NNDSVD\n",
    "W, H = srht_nndsvd_initialization(A, r, rOV, w)\n",
    "\n",
    "# Run NMF\n",
    "model = NMF(n_components=r, init='custom', random_state=0)\n",
    "W_final = model.fit_transform(A, W=W, H=H)\n",
    "H_final = model.components_\n",
    "\n",
    "# Evaluate reconstruction error\n",
    "reconstruction_error = np.linalg.norm(A - W_final @ H_final, ord='fro') / np.linalg.norm(A, ord='fro')\n",
    "print(\"Normalized Reconstruction Error:\", reconstruction_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF Structured Compression (Coord Desc. SKLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_cd(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='cd', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.535564\n",
      "Final W shape: (100, 5)\n",
      "Final H shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_cd(A, r, rOV, w,500)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(10, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m rOV \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m# Oversampling parameter\u001b[39;00m\n\u001b[1;32m     95\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m# Exponent\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m W, H \u001b[38;5;241m=\u001b[39m \u001b[43mnmf_with_fjlt_initialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrOV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal W shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal H shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[24], line 66\u001b[0m, in \u001b[0;36mnmf_with_fjlt_initialization\u001b[0;34m(A, r, rOV, w, max_iter, tol)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(R\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Step 2: Custom initialization for W and H using L and R\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m  \u001b[38;5;66;03m# W ∈ R^(m x d) = (100, 10)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m W \u001b[38;5;241m=\u001b[39m W[:, :r]  \u001b[38;5;66;03m# Take the first r columns for W ∈ R^(m x r) = (100, 5)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m H \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m A  \u001b[38;5;66;03m# H ∈ R^(d x n) = (10, 50)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def fast_jlt_transform(A, target_dim):\n",
    "    \"\"\"\n",
    "    Apply the Fast Johnson-Lindenstrauss Transform (FJLT) to matrix A.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    target_dim (int): Target reduced dimension\n",
    "\n",
    "    Returns:\n",
    "    A_reduced (numpy array): Dimension-reduced matrix (target_dim x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Generate a Hadamard matrix (nearest power of 2)\n",
    "    H_dim = 2**int(np.ceil(np.log2(m)))  # Get the nearest power of 2\n",
    "    H = hadamard(H_dim)[:m, :m]  # Truncate to match dimensions\n",
    "\n",
    "    # Step 2: Create a diagonal sign matrix D (random ±1)\n",
    "    D = np.diag(np.random.choice([-1, 1], size=m))\n",
    "\n",
    "    # Step 3: Apply Hadamard and sign-flipping\n",
    "    HD = H @ D @ A  # (m x n)\n",
    "\n",
    "    # Step 4: Generate a sparse projection matrix P (proper scaling)\n",
    "    P = sparse_random(target_dim, m, density=1/target_dim, format='csr', random_state=42).toarray()\n",
    "    P *= np.sqrt(1 / target_dim)  # Scale projection\n",
    "\n",
    "    # Step 5: Compute the reduced matrix\n",
    "    A_reduced = P @ HD  # (target_dim x n)\n",
    "\n",
    "    return A_reduced\n",
    "\n",
    "def nmf_with_fjlt_initialization(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using FJLT for initialization of L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R using FJLT\n",
    "    L = fast_jlt_transform(A, d)  # L ∈ R^(d x m) = (10, 100)\n",
    "    print(L.shape)\n",
    "    R = fast_jlt_transform(A.T, d)  # R ∈ R^(d x n) = (10, 50)\n",
    "    print(R.shape)\n",
    "    # Step 2: Custom initialization for W and H using L and R\n",
    "    W = A @ R.T  # W ∈ R^(m x d) = (100, 10)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r) = (100, 5)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n) = (10, 50)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n) = (5, 50)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H\n",
    "\n",
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = nmf_with_fjlt_initialization(A, r, rOV, w)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)\n",
    "print(\"W (nonnegative):\\n\", W)\n",
    "print(\"H (nonnegative):\\n\", H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Power iteration exponent\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Initialize NMF with Nyström\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m W_init, H_init \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_nmf_with_nystrom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrOV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run NMF\u001b[39;00m\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m NMF(n_components\u001b[38;5;241m=\u001b[39mr, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m, in \u001b[0;36minitialize_nmf_with_nystrom\u001b[0;34m(A, r, rOV, w)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mInitialize NMF using the Nyström approximation.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Compute Nyström approximation\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m _, U, Sigma, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mnystrom_approximation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrOV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Initialize W and H\u001b[39;00m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m U \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(Sigma)\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mnystrom_approximation\u001b[0;34m(A, r, rOV, w)\u001b[0m\n\u001b[1;32m     16\u001b[0m Omega_L \u001b[38;5;241m=\u001b[39m (F \u001b[38;5;241m@\u001b[39m D)[S, :] \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(d)  \u001b[38;5;66;03m# SRFT matrix\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Step 2: Compute B = (A A^T)^w A Omega_L\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOmega_L\u001b[49m  \u001b[38;5;66;03m# Initial multiplication: A Omega_L\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w):\n\u001b[1;32m     21\u001b[0m     B \u001b[38;5;241m=\u001b[39m (A \u001b[38;5;241m@\u001b[39m A\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m@\u001b[39m B  \u001b[38;5;66;03m# Power iteration: (A A^T) B\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 100)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nystrom_approximation(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute the Nyström approximation of matrix A using SVD.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Generate SRFT matrix\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    F = fft(np.eye(n), axis=0)  # Fourier matrix of size n x n\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (F @ D)[S, :] / np.sqrt(d)  # SRFT matrix\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    # Step 4: Compute C = A^T Q\n",
    "    C = A.T @ Q\n",
    "\n",
    "    # Step 5: Compute the SVD of C\n",
    "    U_c, Sigma_c, V_c = np.linalg.svd(C, full_matrices=False)\n",
    "\n",
    "    # Step 6: Truncate the SVD to the target rank r\n",
    "    U_c = U_c[:, :r]\n",
    "    Sigma_c = Sigma_c[:r]\n",
    "    V_c = V_c[:r, :]\n",
    "\n",
    "    # Step 7: Compute the Nyström approximation\n",
    "    A_approx = Q @ U_c @ np.diag(Sigma_c) @ V_c\n",
    "\n",
    "    return A_approx, U_c, np.diag(Sigma_c), V_c\n",
    "\n",
    "def initialize_nmf_with_nystrom(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Initialize NMF using the Nyström approximation.\n",
    "    \"\"\"\n",
    "    # Compute Nyström approximation\n",
    "    _, U, Sigma, Vt = nystrom_approximation(A, r, rOV, w)\n",
    "\n",
    "    # Initialize W and H\n",
    "    W = U @ np.sqrt(Sigma)\n",
    "    H = np.sqrt(Sigma) @ Vt\n",
    "\n",
    "    # Ensure non-negativity (NMF constraint)\n",
    "    W = np.maximum(W, 0)\n",
    "    H = np.maximum(H, 0)\n",
    "\n",
    "    return W, H\n",
    "\n",
    "# Example usage\n",
    "A = np.random.rand(100, 100)  # Input matrix\n",
    "r = 10  # Target rank\n",
    "rOV = 5  # Oversampling parameter\n",
    "w = 2  # Power iteration exponent\n",
    "\n",
    "# Initialize NMF with Nyström\n",
    "W_init, H_init = initialize_nmf_with_nystrom(A, r, rOV, w)\n",
    "\n",
    "# Run NMF\n",
    "model = NMF(n_components=r, init='custom', random_state=0)\n",
    "W = model.fit_transform(A, W=W_init, H=H_init)\n",
    "H = model.components_\n",
    "\n",
    "print(\"NMF Factor W:\\n\", W)\n",
    "print(\"NMF Factor H:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SNMF using SPA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of extreme columns (K):\n",
      " [np.int64(1), np.int64(28), np.int64(26), np.int64(10), np.int64(16)]\n",
      "Non-negative matrix Y:\n",
      " [[0.26991941 1.         0.32169309 0.3270894  0.07385464 0.17799938\n",
      "  0.12896772 0.17726141 0.15684102 0.04686761 0.         0.27068187\n",
      "  0.11111135 0.10668048 0.3301494  0.22857608 0.         0.13683459\n",
      "  0.19878897 0.12512137 0.21008757 0.20260434 0.20696132 0.17159382\n",
      "  0.22993802 0.25569056 0.         0.17877939 0.         0.10643334\n",
      "  0.2067115  0.30505273 0.24173546 0.13249975 0.19917636 0.46016169\n",
      "  0.28468756 0.26616789 0.13223317 0.29455146 0.25708465 0.18579592\n",
      "  0.17917939 0.35785239 0.25395612 0.20075296 0.35091016 0.24745551\n",
      "  0.26990902 0.11251674]\n",
      " [0.12728463 0.         0.13077099 0.15571189 0.14138647 0.33393456\n",
      "  0.27494749 0.20558222 0.18048183 0.14265413 0.         0.25345983\n",
      "  0.14655855 0.22710121 0.17994212 0.11178959 0.         0.14861172\n",
      "  0.2237423  0.26145654 0.19402906 0.21099129 0.17461339 0.16012623\n",
      "  0.13967756 0.13891845 0.         0.12512735 1.         0.24035249\n",
      "  0.14860806 0.08300927 0.08754898 0.27632582 0.14423461 0.20012301\n",
      "  0.20290265 0.037654   0.144624   0.2045577  0.10204326 0.\n",
      "  0.08795492 0.12624293 0.13189966 0.1332532  0.14756035 0.09728452\n",
      "  0.04492725 0.15900433]\n",
      " [0.25145873 0.         0.01809653 0.         0.15089917 0.0816754\n",
      "  0.10417278 0.         0.08915285 0.23136951 0.         0.11963775\n",
      "  0.08345882 0.23536013 0.11478488 0.18211081 0.         0.11690061\n",
      "  0.25105932 0.30308564 0.07005857 0.13933346 0.09911003 0.27818876\n",
      "  0.07792495 0.18397676 1.         0.21606222 0.         0.2632604\n",
      "  0.2326116  0.22084389 0.08231619 0.23154111 0.0397074  0.0584377\n",
      "  0.22285316 0.10177399 0.05086972 0.         0.11625215 0.30471989\n",
      "  0.20043735 0.04367905 0.14044625 0.18535598 0.         0.27728878\n",
      "  0.20583166 0.27987464]\n",
      " [0.11434711 0.         0.35905296 0.34143465 0.23269808 0.24085656\n",
      "  0.33646422 0.32040479 0.23881719 0.23904376 1.         0.26864577\n",
      "  0.29051309 0.2349708  0.33237852 0.36311001 0.         0.2378127\n",
      "  0.0426439  0.20109001 0.27818491 0.21032118 0.43431886 0.20419239\n",
      "  0.3292978  0.22517048 0.         0.334552   0.         0.38412646\n",
      "  0.35062893 0.21402165 0.28459719 0.12276526 0.27811029 0.26773581\n",
      "  0.19990851 0.33280649 0.12977086 0.26259591 0.24889854 0.35467865\n",
      "  0.12313308 0.33284338 0.27461995 0.15507538 0.22080032 0.2057338\n",
      "  0.42508679 0.30988244]\n",
      " [0.1931185  0.         0.15889366 0.13216971 0.32808851 0.24206938\n",
      "  0.13957731 0.26660567 0.30531903 0.22710966 0.         0.08267535\n",
      "  0.24401211 0.14365268 0.111865   0.11467545 1.         0.18351041\n",
      "  0.26448588 0.09708983 0.22831036 0.19264396 0.12185501 0.16538229\n",
      "  0.23271733 0.22386608 0.         0.15380408 0.         0.08317329\n",
      "  0.05399848 0.18413132 0.27513071 0.13146747 0.34593644 0.07865634\n",
      "  0.12871776 0.19918199 0.46119256 0.1998611  0.11551463 0.18920792\n",
      "  0.40753334 0.2514407  0.33173565 0.22982097 0.30187956 0.18126205\n",
      "  0.12060117 0.16157209]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_compression_matrix(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute the compression matrix Q using randomized sketching.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for power iterations\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Draw a Gaussian random matrix\n",
    "    Omega_L = np.random.randn(n, d)\n",
    "\n",
    "    # Step 2: Form the matrix product B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def successive_projection_algorithm(R, r):\n",
    "    \"\"\"\n",
    "    Successive Projection Algorithm (SPA) to find extreme columns.\n",
    "\n",
    "    Parameters:\n",
    "    R (numpy array): Projected matrix (d x n), where d = r + rOV\n",
    "    r (int): Number of extreme columns to find\n",
    "\n",
    "    Returns:\n",
    "    K (list): Indices of the extreme columns\n",
    "    \"\"\"\n",
    "    d, n = R.shape\n",
    "\n",
    "    # Normalize the columns of R\n",
    "    R_norm = R / np.linalg.norm(R, axis=0)\n",
    "\n",
    "    # Initialize the set of extreme columns\n",
    "    K = []\n",
    "    R_res = R_norm.copy()  # Residual matrix\n",
    "\n",
    "    for _ in range(r):\n",
    "        # Select the column with the largest norm\n",
    "        norms = np.linalg.norm(R_res, axis=0)\n",
    "        k = np.argmax(norms)\n",
    "        K.append(k)\n",
    "\n",
    "        # Update the residual matrix\n",
    "        w = R_res[:, k].reshape(-1, 1)\n",
    "        R_res = R_res - w @ (w.T @ R_res)\n",
    "\n",
    "    return K\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "def solve_for_Y(A, K):\n",
    "    \"\"\"\n",
    "    Solve for the non-negative matrix Y using NNLS.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    K (list): Indices of the extreme columns\n",
    "\n",
    "    Returns:\n",
    "    Y (numpy array): Non-negative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    r = len(K)\n",
    "\n",
    "    # Extract the extreme columns\n",
    "    W = A[:, K]\n",
    "\n",
    "    # Solve for Y using NNLS\n",
    "    Y = np.zeros((r, n))\n",
    "    for i in range(n):\n",
    "        Y[:, i], _ = nnls(W, A[:, i])\n",
    "\n",
    "    return Y\n",
    "\n",
    "def randomized_spa_nmf(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Randomized SPA for NMF.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for power iterations\n",
    "\n",
    "    Returns:\n",
    "    K (list): Indices of the extreme columns\n",
    "    Y (numpy array): Non-negative matrix (r x n)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute compression matrix Q\n",
    "    Q = compute_compression_matrix(A, r, rOV, w)\n",
    "\n",
    "    # Step 2: Find extreme columns using SPA\n",
    "    R = Q.T @ A\n",
    "    K = successive_projection_algorithm(R, r)\n",
    "\n",
    "    # Step 3: Solve for Y using NNLS\n",
    "    Y = solve_for_Y(A, K)\n",
    "\n",
    "    return K, Y\n",
    "\n",
    "# Example usage\n",
    "A = np.random.rand(100, 50)  # Input matrix (m x n)\n",
    "r = 5  # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2  # Exponent for power iterations\n",
    "\n",
    "K, Y = randomized_spa_nmf(A, r, rOV, w)\n",
    "print(\"Indices of extreme columns (K):\\n\", K)\n",
    "print(\"Non-negative matrix Y:\\n\", Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (128,) into shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Input matrix (m x n)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Step 1: Apply Haar Transform to A\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m A_haar \u001b[38;5;241m=\u001b[39m \u001b[43mapply_haar_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Step 2: Apply NMF to the transformed matrix\u001b[39;00m\n\u001b[1;32m     44\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Target rank\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mapply_haar_to_matrix\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     32\u001b[0m A_haar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(A)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mA_haar\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m haar_transform(A[:, j])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A_haar\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (128,) into shape (100,)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def haar_transform(x):\n",
    "    \"\"\"\n",
    "    Compute the Haar Transform of a 1D signal.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n == 1:\n",
    "        return x\n",
    "    \n",
    "    # Pad the array with zeros if its length is odd\n",
    "    if n % 2 != 0:\n",
    "        x = np.append(x, 0)\n",
    "        n += 1\n",
    "    \n",
    "    even = x[0::2]  # Even-indexed elements\n",
    "    odd = x[1::2]   # Odd-indexed elements\n",
    "    \n",
    "    # Compute the average and difference\n",
    "    avg = (even + odd) / np.sqrt(2)\n",
    "    diff = (even - odd) / np.sqrt(2)\n",
    "    \n",
    "    # Recursively apply the Haar Transform\n",
    "    return np.concatenate([haar_transform(avg), haar_transform(diff)])\n",
    "\n",
    "def apply_haar_to_matrix(A):\n",
    "    \"\"\"\n",
    "    Apply the Haar Transform to each column of a matrix.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    A_haar = np.zeros_like(A)\n",
    "    for j in range(n):\n",
    "        A_haar[:, j] = haar_transform(A[:, j])\n",
    "    return A_haar\n",
    "\n",
    "# Example usage\n",
    "A = np.random.rand(100, 50)  # Input matrix (m x n)\n",
    "\n",
    "# Step 1: Apply Haar Transform to A\n",
    "A_haar = apply_haar_to_matrix(A)\n",
    "\n",
    "# Step 2: Apply NMF to the transformed matrix\n",
    "r = 5  # Target rank\n",
    "model = NMF(n_components=r, init='random', random_state=0)\n",
    "W = model.fit_transform(A_haar)\n",
    "H = model.components_\n",
    "\n",
    "print(\"Factor matrix W:\\n\", W)\n",
    "print(\"Factor matrix H:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CWT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression matrix Q:\n",
      " [[-0.09757166 -0.1102952  -0.07591171 ... -0.04338793  0.0531272\n",
      "   0.14213953]\n",
      " [-0.10398852 -0.07738298 -0.16915485 ...  0.00850355  0.03856103\n",
      "  -0.07143917]\n",
      " [-0.098821    0.03653538 -0.17303241 ...  0.0366262  -0.06850158\n",
      "  -0.01135614]\n",
      " ...\n",
      " [-0.10781335 -0.11737361  0.00437076 ... -0.07340671 -0.037881\n",
      "   0.01851559]\n",
      " [-0.10337914  0.15352473  0.02710434 ... -0.10938549  0.02912627\n",
      "   0.08336967]\n",
      " [-0.10388371  0.06883045  0.04561477 ...  0.04883183  0.01642673\n",
      "  -0.05629869]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cwt(A, k):\n",
    "    \"\"\"\n",
    "    Clarkson-Woodruff Transform (CWT).\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    k (int): Number of rows in the sketch\n",
    "\n",
    "    Returns:\n",
    "    R (numpy array): Sketch of A (k x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Random hashing\n",
    "    h = np.random.randint(0, k, size=n)  # Hash each column to a random bucket\n",
    "    s = np.random.choice([-1, 1], size=n)  # Random sign flips\n",
    "\n",
    "    # Step 2: Construct the sketch\n",
    "    R = np.zeros((k, m))  # Sketch has dimensions (k x m)\n",
    "    for j in range(n):\n",
    "        R[h[j], :] += s[j] * A[:, j]  # Add the scaled column to the hashed row\n",
    "\n",
    "    return R\n",
    "\n",
    "def randomized_compression_cwt(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using CWT-based randomized algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Use CWT to construct Omega_L\n",
    "    Omega_L = cwt(A.T, d).T  # Apply CWT to A^T and transpose to get (n x d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Example usage\n",
    "A = np.abs(np.random.rand(100, 50)) # Input matrix (m x n)\n",
    "r = 5  # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2  # Exponent for power iterations\n",
    "\n",
    "Q = randomized_compression_cwt(A, r, rOV, w)\n",
    "print(\"Compression matrix Q:\\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
