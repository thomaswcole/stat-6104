{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_compression(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using a randomized algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Draw a Gaussian random matrix Omega_L\n",
    "    Omega_L = np.random.randn(n, d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = A @ (A.T @ B)  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression matrix Q shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(100, 50)  # Input matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Q = randomized_compression(A, r, rOV, w)\n",
    "print(\"Compression matrix Q shape:\", Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Random Compression Fig 2 (Multiplicative Updates)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_with_compression(A, r, rOV, w, max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Compute NMF with compression matrices L and R using multiplicative updates.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    X_k (numpy array): Nonnegative matrix (m x r)\n",
    "    Y_k (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Initialize Y_k with nonnegative values\n",
    "    Y_k = np.abs(np.random.randn(r, n))  # Y_k ∈ R^(r x n)\n",
    "\n",
    "    # Step 3: Compute compressed matrices\n",
    "    A_check = A @ R.T  # A_check ∈ R^(m x d)\n",
    "    A_hat = L.T @ A  # A_hat ∈ R^(d x n)\n",
    "\n",
    "    # Ensure compressed matrices are nonnegative\n",
    "    A_check = np.abs(A_check)\n",
    "    A_hat = np.abs(A_hat)\n",
    "\n",
    "    # Initialize X_k\n",
    "    X_k = np.abs(np.random.randn(m, r))  # X_k ∈ R^(m x r)\n",
    "\n",
    "    norm_A = np.linalg.norm(A, 'fro')\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    prev_error = np.inf\n",
    "    for k in range(max_iter):\n",
    "        # Step 4: Compute Y_check_k = Y_k R^T\n",
    "        Y_check_k = Y_k @ R.T  # Y_check_k ∈ R^(r x d)\n",
    "\n",
    "        # Step 5: Update X_k+1 using multiplicative updates\n",
    "        numerator_X = A_check @ Y_check_k.T\n",
    "        denominator_X = X_k @ (Y_check_k @ Y_check_k.T)\n",
    "        X_k_plus_1 = X_k * (numerator_X / denominator_X)\n",
    "\n",
    "        # Step 6: Compute X_hat_k+1 = L^T X_k+1\n",
    "        X_hat_k_plus_1 = L.T @ X_k_plus_1  # X_hat_k+1 ∈ R^(d x r)\n",
    "\n",
    "        # Step 7: Update Y_k+1 using multiplicative updates\n",
    "        numerator_Y = X_hat_k_plus_1.T @ A_hat\n",
    "        denominator_Y = (X_hat_k_plus_1.T @ X_hat_k_plus_1) @ Y_k\n",
    "        Y_k_plus_1 = Y_k * (numerator_Y / denominator_Y)\n",
    "\n",
    "        # Compute reconstruction error\n",
    "        reconstruction_error = np.linalg.norm(A - X_k_plus_1 @ Y_k_plus_1, 'fro')\n",
    "        normalized_error = reconstruction_error / norm_A  # Normalized error\n",
    "        print(f\"Iteration {k+1}: Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if abs(prev_error - normalized_error) < tol:\n",
    "            break\n",
    "        prev_error = normalized_error\n",
    "\n",
    "\n",
    "        # Update X_k and Y_k\n",
    "        X_k, Y_k = X_k_plus_1, Y_k_plus_1\n",
    "\n",
    "    return X_k, Y_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Normalized Reconstruction Error = 1.719811\n",
      "Iteration 2: Normalized Reconstruction Error = 1.719563\n",
      "Iteration 3: Normalized Reconstruction Error = 1.719371\n",
      "Iteration 4: Normalized Reconstruction Error = 1.719237\n",
      "Iteration 5: Normalized Reconstruction Error = 1.719153\n",
      "Iteration 6: Normalized Reconstruction Error = 1.719108\n",
      "Iteration 7: Normalized Reconstruction Error = 1.719097\n",
      "Iteration 8: Normalized Reconstruction Error = 1.719112\n",
      "Iteration 9: Normalized Reconstruction Error = 1.719151\n",
      "Iteration 10: Normalized Reconstruction Error = 1.719209\n",
      "Iteration 11: Normalized Reconstruction Error = 1.719284\n",
      "Iteration 12: Normalized Reconstruction Error = 1.719374\n",
      "Iteration 13: Normalized Reconstruction Error = 1.719476\n",
      "Iteration 14: Normalized Reconstruction Error = 1.719589\n",
      "Iteration 15: Normalized Reconstruction Error = 1.719710\n",
      "Iteration 16: Normalized Reconstruction Error = 1.719838\n",
      "Iteration 17: Normalized Reconstruction Error = 1.719971\n",
      "Iteration 18: Normalized Reconstruction Error = 1.720108\n",
      "Iteration 19: Normalized Reconstruction Error = 1.720246\n",
      "Iteration 20: Normalized Reconstruction Error = 1.720384\n",
      "Iteration 21: Normalized Reconstruction Error = 1.720520\n",
      "Iteration 22: Normalized Reconstruction Error = 1.720653\n",
      "Iteration 23: Normalized Reconstruction Error = 1.720782\n",
      "Iteration 24: Normalized Reconstruction Error = 1.720906\n",
      "Iteration 25: Normalized Reconstruction Error = 1.721023\n",
      "Iteration 26: Normalized Reconstruction Error = 1.721132\n",
      "Iteration 27: Normalized Reconstruction Error = 1.721233\n",
      "Iteration 28: Normalized Reconstruction Error = 1.721325\n",
      "Iteration 29: Normalized Reconstruction Error = 1.721408\n",
      "Iteration 30: Normalized Reconstruction Error = 1.721482\n",
      "Iteration 31: Normalized Reconstruction Error = 1.721546\n",
      "Iteration 32: Normalized Reconstruction Error = 1.721602\n",
      "Iteration 33: Normalized Reconstruction Error = 1.721649\n",
      "Iteration 34: Normalized Reconstruction Error = 1.721689\n",
      "Iteration 35: Normalized Reconstruction Error = 1.721722\n",
      "Iteration 36: Normalized Reconstruction Error = 1.721749\n",
      "Iteration 37: Normalized Reconstruction Error = 1.721772\n",
      "Iteration 38: Normalized Reconstruction Error = 1.721790\n",
      "Iteration 39: Normalized Reconstruction Error = 1.721807\n",
      "Iteration 40: Normalized Reconstruction Error = 1.721822\n",
      "Iteration 41: Normalized Reconstruction Error = 1.721837\n",
      "Iteration 42: Normalized Reconstruction Error = 1.721853\n",
      "Iteration 43: Normalized Reconstruction Error = 1.721871\n",
      "Iteration 44: Normalized Reconstruction Error = 1.721891\n",
      "Iteration 45: Normalized Reconstruction Error = 1.721913\n",
      "Iteration 46: Normalized Reconstruction Error = 1.721939\n",
      "Iteration 47: Normalized Reconstruction Error = 1.721966\n",
      "Iteration 48: Normalized Reconstruction Error = 1.721995\n",
      "Iteration 49: Normalized Reconstruction Error = 1.722023\n",
      "Iteration 50: Normalized Reconstruction Error = 1.722047\n",
      "Iteration 51: Normalized Reconstruction Error = 1.722063\n",
      "Iteration 52: Normalized Reconstruction Error = 1.722068\n",
      "Iteration 53: Normalized Reconstruction Error = 1.722059\n",
      "Iteration 54: Normalized Reconstruction Error = 1.722034\n",
      "Iteration 55: Normalized Reconstruction Error = 1.721993\n",
      "Iteration 56: Normalized Reconstruction Error = 1.721935\n",
      "Iteration 57: Normalized Reconstruction Error = 1.721863\n",
      "Iteration 58: Normalized Reconstruction Error = 1.721778\n",
      "Iteration 59: Normalized Reconstruction Error = 1.721681\n",
      "Iteration 60: Normalized Reconstruction Error = 1.721575\n",
      "Iteration 61: Normalized Reconstruction Error = 1.721461\n",
      "Iteration 62: Normalized Reconstruction Error = 1.721339\n",
      "Iteration 63: Normalized Reconstruction Error = 1.721213\n",
      "Iteration 64: Normalized Reconstruction Error = 1.721082\n",
      "Iteration 65: Normalized Reconstruction Error = 1.720949\n",
      "Iteration 66: Normalized Reconstruction Error = 1.720813\n",
      "Iteration 67: Normalized Reconstruction Error = 1.720676\n",
      "Iteration 68: Normalized Reconstruction Error = 1.720538\n",
      "Iteration 69: Normalized Reconstruction Error = 1.720400\n",
      "Iteration 70: Normalized Reconstruction Error = 1.720262\n",
      "Iteration 71: Normalized Reconstruction Error = 1.720124\n",
      "Iteration 72: Normalized Reconstruction Error = 1.719987\n",
      "Iteration 73: Normalized Reconstruction Error = 1.719851\n",
      "Iteration 74: Normalized Reconstruction Error = 1.719716\n",
      "Iteration 75: Normalized Reconstruction Error = 1.719581\n",
      "Iteration 76: Normalized Reconstruction Error = 1.719448\n",
      "Iteration 77: Normalized Reconstruction Error = 1.719315\n",
      "Iteration 78: Normalized Reconstruction Error = 1.719182\n",
      "Iteration 79: Normalized Reconstruction Error = 1.719050\n",
      "Iteration 80: Normalized Reconstruction Error = 1.718919\n",
      "Iteration 81: Normalized Reconstruction Error = 1.718787\n",
      "Iteration 82: Normalized Reconstruction Error = 1.718656\n",
      "Iteration 83: Normalized Reconstruction Error = 1.718525\n",
      "Iteration 84: Normalized Reconstruction Error = 1.718395\n",
      "Iteration 85: Normalized Reconstruction Error = 1.718266\n",
      "Iteration 86: Normalized Reconstruction Error = 1.718138\n",
      "Iteration 87: Normalized Reconstruction Error = 1.718013\n",
      "Iteration 88: Normalized Reconstruction Error = 1.717891\n",
      "Iteration 89: Normalized Reconstruction Error = 1.717772\n",
      "Iteration 90: Normalized Reconstruction Error = 1.717657\n",
      "Iteration 91: Normalized Reconstruction Error = 1.717547\n",
      "Iteration 92: Normalized Reconstruction Error = 1.717442\n",
      "Iteration 93: Normalized Reconstruction Error = 1.717342\n",
      "Iteration 94: Normalized Reconstruction Error = 1.717248\n",
      "Iteration 95: Normalized Reconstruction Error = 1.717160\n",
      "Iteration 96: Normalized Reconstruction Error = 1.717078\n",
      "Iteration 97: Normalized Reconstruction Error = 1.717001\n",
      "Iteration 98: Normalized Reconstruction Error = 1.716930\n",
      "Iteration 99: Normalized Reconstruction Error = 1.716865\n",
      "Iteration 100: Normalized Reconstruction Error = 1.716804\n",
      "Final X shape: (100, 5)\n",
      "Final Y shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Xk, Yk = nmf_with_compression(A, r, rOV, w)\n",
    "print(\"Final X shape:\", Xk.shape)\n",
    "print(\"Final Y shape:\", Yk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Compression (MU SkLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_mu(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.550010\n",
      "Final W shape: (100, 5)\n",
      "Final H shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_mu(A, r, rOV, w)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF Structured Compression (Coord Desc. SKLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_cd(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='cd', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.535564\n",
      "Final W shape: (100, 5)\n",
      "Final H shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_cd(A, r, rOV, w,500)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(10, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m rOV \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m# Oversampling parameter\u001b[39;00m\n\u001b[1;32m     95\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m# Exponent\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m W, H \u001b[38;5;241m=\u001b[39m \u001b[43mnmf_with_fjlt_initialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrOV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal W shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal H shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[24], line 66\u001b[0m, in \u001b[0;36mnmf_with_fjlt_initialization\u001b[0;34m(A, r, rOV, w, max_iter, tol)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(R\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Step 2: Custom initialization for W and H using L and R\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m  \u001b[38;5;66;03m# W ∈ R^(m x d) = (100, 10)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m W \u001b[38;5;241m=\u001b[39m W[:, :r]  \u001b[38;5;66;03m# Take the first r columns for W ∈ R^(m x r) = (100, 5)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m H \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m A  \u001b[38;5;66;03m# H ∈ R^(d x n) = (10, 50)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def fast_jlt_transform(A, target_dim):\n",
    "    \"\"\"\n",
    "    Apply the Fast Johnson-Lindenstrauss Transform (FJLT) to matrix A.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    target_dim (int): Target reduced dimension\n",
    "\n",
    "    Returns:\n",
    "    A_reduced (numpy array): Dimension-reduced matrix (target_dim x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Generate a Hadamard matrix (nearest power of 2)\n",
    "    H_dim = 2**int(np.ceil(np.log2(m)))  # Get the nearest power of 2\n",
    "    H = hadamard(H_dim)[:m, :m]  # Truncate to match dimensions\n",
    "\n",
    "    # Step 2: Create a diagonal sign matrix D (random ±1)\n",
    "    D = np.diag(np.random.choice([-1, 1], size=m))\n",
    "\n",
    "    # Step 3: Apply Hadamard and sign-flipping\n",
    "    HD = H @ D @ A  # (m x n)\n",
    "\n",
    "    # Step 4: Generate a sparse projection matrix P (proper scaling)\n",
    "    P = sparse_random(target_dim, m, density=1/target_dim, format='csr', random_state=42).toarray()\n",
    "    P *= np.sqrt(1 / target_dim)  # Scale projection\n",
    "\n",
    "    # Step 5: Compute the reduced matrix\n",
    "    A_reduced = P @ HD  # (target_dim x n)\n",
    "\n",
    "    return A_reduced\n",
    "\n",
    "def nmf_with_fjlt_initialization(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using FJLT for initialization of L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R using FJLT\n",
    "    L = fast_jlt_transform(A, d)  # L ∈ R^(d x m) = (10, 100)\n",
    "    print(L.shape)\n",
    "    R = fast_jlt_transform(A.T, d)  # R ∈ R^(d x n) = (10, 50)\n",
    "    print(R.shape)\n",
    "    # Step 2: Custom initialization for W and H using L and R\n",
    "    W = A @ R.T  # W ∈ R^(m x d) = (100, 10)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r) = (100, 5)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n) = (10, 50)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n) = (5, 50)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H\n",
    "\n",
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = nmf_with_fjlt_initialization(A, r, rOV, w)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)\n",
    "print(\"W (nonnegative):\\n\", W)\n",
    "print(\"H (nonnegative):\\n\", H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
