{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_compression(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using a randomized algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Draw a Gaussian random matrix Omega_L\n",
    "    Omega_L = np.random.randn(n, d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression matrix Q shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(100, 50)  # Input matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Q = randomized_compression(A, r, rOV, w)\n",
    "print(\"Compression matrix Q shape:\", Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1 SRHT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "\n",
    "def randomized_compression_srht(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using SRHT as the test matrix.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure n is a power of 2 by padding A with zeros if necessary\n",
    "    n_padded = 2 ** int(np.ceil(np.log2(n)))  # Smallest power of 2 >= n\n",
    "    if n_padded != n:\n",
    "        A_padded = np.zeros((m, n_padded))\n",
    "        A_padded[:, :n] = A  # Pad with zeros\n",
    "        A = A_padded\n",
    "        n = n_padded\n",
    "\n",
    "    # Step 1: Generate SRHT matrix\n",
    "    H = hadamard(n)  # Hadamard matrix of size n x n\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (H @ D)[:, S] / np.sqrt(d)  # SRHT matrix (n x d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: (1000, 60)\n",
      "Orthogonality check (Q^T Q):\n",
      "[[ 1.  0.  0. ...  0.  0. -0.]\n",
      " [ 0.  1. -0. ... -0.  0.  0.]\n",
      " [ 0. -0.  1. ... -0. -0.  0.]\n",
      " ...\n",
      " [ 0. -0. -0. ...  1.  0.  0.]\n",
      " [ 0.  0. -0. ...  0.  1.  0.]\n",
      " [-0.  0.  0. ...  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of the input matrix A\n",
    "m = 1000  # Number of rows\n",
    "n = 500   # Number of columns\n",
    "\n",
    "# Generate a random matrix A ∈ R^(m x n)\n",
    "A = np.random.randn(m, n)\n",
    "\n",
    "# Set the parameters for the randomized compression\n",
    "r = 50    # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2     # Exponent for power iteration\n",
    "\n",
    "# Call the SRHT-based randomized compression function\n",
    "Q = randomized_compression_srht(A, r, rOV, w)\n",
    "\n",
    "# Print the shape of the resulting compression matrix Q\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected output: (1000, 60) since r + rOV = 50 + 10 = 60\n",
    "\n",
    "# Verify that Q is orthogonal (Q^T Q ≈ I)\n",
    "orthogonality_check = Q.T @ Q\n",
    "print(\"Orthogonality check (Q^T Q):\")\n",
    "print(np.round(orthogonality_check, 6))  # Should be close to the identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Random Compression Algorithm Fig 1 SRFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "def randomized_compression_srft(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a compression matrix Q for A using SRFT as the test matrix.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "\n",
    "    Returns:\n",
    "    Q (numpy array): Compression matrix (m x (r + rOV))\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Step 1: Generate SRFT matrix\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    F = fft(np.eye(n), axis=0)  # Fourier matrix of size n x n\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (F @ D)[S, :] / np.sqrt(d)  # SRFT matrix\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Random Compression Fig 2 (Multiplicative Updates)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_with_compression(A, r, rOV, w, max_iter=100, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Compute NMF with compression matrices L and R using multiplicative updates.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    X_k (numpy array): Nonnegative matrix (m x r)\n",
    "    Y_k (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression_srht(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression_srht(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Initialize Y_k with nonnegative values\n",
    "    Y_k = np.abs(np.random.randn(r, n))  # Y_k ∈ R^(r x n)\n",
    "\n",
    "    # Step 3: Compute compressed matrices\n",
    "    A_check = A @ R.T  # A_check ∈ R^(m x d)\n",
    "    A_hat = L.T @ A  # A_hat ∈ R^(d x n)\n",
    "\n",
    "    # Ensure compressed matrices are nonnegative\n",
    "    A_check = np.abs(A_check)\n",
    "    A_hat = np.abs(A_hat)\n",
    "\n",
    "    # Initialize X_k\n",
    "    X_k = np.abs(np.random.randn(m, r))  # X_k ∈ R^(m x r)\n",
    "\n",
    "    norm_A = np.linalg.norm(A, 'fro')\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    prev_error = np.inf\n",
    "    for k in range(max_iter):\n",
    "        # Step 4: Compute Y_check_k = Y_k R^T\n",
    "        Y_check_k = Y_k @ R.T  # Y_check_k ∈ R^(r x d)\n",
    "\n",
    "        # Step 5: Update X_k+1 using multiplicative updates\n",
    "        numerator_X = A_check @ Y_check_k.T\n",
    "        denominator_X = X_k @ (Y_check_k @ Y_check_k.T)\n",
    "        X_k_plus_1 = X_k * (numerator_X / denominator_X)\n",
    "\n",
    "        # Step 6: Compute X_hat_k+1 = L^T X_k+1\n",
    "        X_hat_k_plus_1 = L.T @ X_k_plus_1  # X_hat_k+1 ∈ R^(d x r)\n",
    "\n",
    "        # Step 7: Update Y_k+1 using multiplicative updates\n",
    "        numerator_Y = X_hat_k_plus_1.T @ A_hat\n",
    "        denominator_Y = (X_hat_k_plus_1.T @ X_hat_k_plus_1) @ Y_k\n",
    "        Y_k_plus_1 = Y_k * (numerator_Y / denominator_Y)\n",
    "\n",
    "        # Compute reconstruction error\n",
    "        reconstruction_error = np.linalg.norm(A - X_k_plus_1 @ Y_k_plus_1, 'fro')\n",
    "        normalized_error = reconstruction_error / norm_A  # Normalized error\n",
    "        print(f\"Iteration {k+1}: Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if abs(prev_error - normalized_error) < tol:\n",
    "            break\n",
    "        prev_error = normalized_error\n",
    "\n",
    "\n",
    "        # Update X_k and Y_k\n",
    "        X_k, Y_k = X_k_plus_1, Y_k_plus_1\n",
    "\n",
    "    return X_k, Y_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Normalized Reconstruction Error = 1.721853\n",
      "Iteration 2: Normalized Reconstruction Error = 1.721625\n",
      "Iteration 3: Normalized Reconstruction Error = 1.721383\n",
      "Iteration 4: Normalized Reconstruction Error = 1.721198\n",
      "Iteration 5: Normalized Reconstruction Error = 1.721072\n",
      "Iteration 6: Normalized Reconstruction Error = 1.721001\n",
      "Iteration 7: Normalized Reconstruction Error = 1.720982\n",
      "Iteration 8: Normalized Reconstruction Error = 1.721009\n",
      "Iteration 9: Normalized Reconstruction Error = 1.721076\n",
      "Iteration 10: Normalized Reconstruction Error = 1.721178\n",
      "Iteration 11: Normalized Reconstruction Error = 1.721309\n",
      "Iteration 12: Normalized Reconstruction Error = 1.721461\n",
      "Iteration 13: Normalized Reconstruction Error = 1.721631\n",
      "Iteration 14: Normalized Reconstruction Error = 1.721814\n",
      "Iteration 15: Normalized Reconstruction Error = 1.722006\n",
      "Iteration 16: Normalized Reconstruction Error = 1.722202\n",
      "Iteration 17: Normalized Reconstruction Error = 1.722402\n",
      "Iteration 18: Normalized Reconstruction Error = 1.722601\n",
      "Iteration 19: Normalized Reconstruction Error = 1.722800\n",
      "Iteration 20: Normalized Reconstruction Error = 1.722995\n",
      "Iteration 21: Normalized Reconstruction Error = 1.723187\n",
      "Iteration 22: Normalized Reconstruction Error = 1.723373\n",
      "Iteration 23: Normalized Reconstruction Error = 1.723555\n",
      "Iteration 24: Normalized Reconstruction Error = 1.723730\n",
      "Iteration 25: Normalized Reconstruction Error = 1.723900\n",
      "Iteration 26: Normalized Reconstruction Error = 1.724063\n",
      "Iteration 27: Normalized Reconstruction Error = 1.724220\n",
      "Iteration 28: Normalized Reconstruction Error = 1.724370\n",
      "Iteration 29: Normalized Reconstruction Error = 1.724513\n",
      "Iteration 30: Normalized Reconstruction Error = 1.724650\n",
      "Iteration 31: Normalized Reconstruction Error = 1.724780\n",
      "Iteration 32: Normalized Reconstruction Error = 1.724903\n",
      "Iteration 33: Normalized Reconstruction Error = 1.725018\n",
      "Iteration 34: Normalized Reconstruction Error = 1.725127\n",
      "Iteration 35: Normalized Reconstruction Error = 1.725228\n",
      "Iteration 36: Normalized Reconstruction Error = 1.725321\n",
      "Iteration 37: Normalized Reconstruction Error = 1.725408\n",
      "Iteration 38: Normalized Reconstruction Error = 1.725486\n",
      "Iteration 39: Normalized Reconstruction Error = 1.725558\n",
      "Iteration 40: Normalized Reconstruction Error = 1.725622\n",
      "Iteration 41: Normalized Reconstruction Error = 1.725680\n",
      "Iteration 42: Normalized Reconstruction Error = 1.725731\n",
      "Iteration 43: Normalized Reconstruction Error = 1.725775\n",
      "Iteration 44: Normalized Reconstruction Error = 1.725813\n",
      "Iteration 45: Normalized Reconstruction Error = 1.725846\n",
      "Iteration 46: Normalized Reconstruction Error = 1.725872\n",
      "Iteration 47: Normalized Reconstruction Error = 1.725894\n",
      "Iteration 48: Normalized Reconstruction Error = 1.725911\n",
      "Iteration 49: Normalized Reconstruction Error = 1.725923\n",
      "Iteration 50: Normalized Reconstruction Error = 1.725931\n",
      "Iteration 51: Normalized Reconstruction Error = 1.725935\n",
      "Iteration 52: Normalized Reconstruction Error = 1.725937\n",
      "Iteration 53: Normalized Reconstruction Error = 1.725936\n",
      "Iteration 54: Normalized Reconstruction Error = 1.725933\n",
      "Iteration 55: Normalized Reconstruction Error = 1.725928\n",
      "Iteration 56: Normalized Reconstruction Error = 1.725923\n",
      "Iteration 57: Normalized Reconstruction Error = 1.725919\n",
      "Iteration 58: Normalized Reconstruction Error = 1.725915\n",
      "Iteration 59: Normalized Reconstruction Error = 1.725912\n",
      "Iteration 60: Normalized Reconstruction Error = 1.725912\n",
      "Iteration 61: Normalized Reconstruction Error = 1.725914\n",
      "Iteration 62: Normalized Reconstruction Error = 1.725920\n",
      "Iteration 63: Normalized Reconstruction Error = 1.725930\n",
      "Iteration 64: Normalized Reconstruction Error = 1.725944\n",
      "Iteration 65: Normalized Reconstruction Error = 1.725964\n",
      "Iteration 66: Normalized Reconstruction Error = 1.725989\n",
      "Iteration 67: Normalized Reconstruction Error = 1.726019\n",
      "Iteration 68: Normalized Reconstruction Error = 1.726056\n",
      "Iteration 69: Normalized Reconstruction Error = 1.726098\n",
      "Iteration 70: Normalized Reconstruction Error = 1.726146\n",
      "Iteration 71: Normalized Reconstruction Error = 1.726200\n",
      "Iteration 72: Normalized Reconstruction Error = 1.726258\n",
      "Iteration 73: Normalized Reconstruction Error = 1.726321\n",
      "Iteration 74: Normalized Reconstruction Error = 1.726387\n",
      "Iteration 75: Normalized Reconstruction Error = 1.726456\n",
      "Iteration 76: Normalized Reconstruction Error = 1.726528\n",
      "Iteration 77: Normalized Reconstruction Error = 1.726600\n",
      "Iteration 78: Normalized Reconstruction Error = 1.726673\n",
      "Iteration 79: Normalized Reconstruction Error = 1.726747\n",
      "Iteration 80: Normalized Reconstruction Error = 1.726820\n",
      "Iteration 81: Normalized Reconstruction Error = 1.726892\n",
      "Iteration 82: Normalized Reconstruction Error = 1.726963\n",
      "Iteration 83: Normalized Reconstruction Error = 1.727032\n",
      "Iteration 84: Normalized Reconstruction Error = 1.727100\n",
      "Iteration 85: Normalized Reconstruction Error = 1.727167\n",
      "Iteration 86: Normalized Reconstruction Error = 1.727232\n",
      "Iteration 87: Normalized Reconstruction Error = 1.727295\n",
      "Iteration 88: Normalized Reconstruction Error = 1.727357\n",
      "Iteration 89: Normalized Reconstruction Error = 1.727419\n",
      "Iteration 90: Normalized Reconstruction Error = 1.727480\n",
      "Iteration 91: Normalized Reconstruction Error = 1.727541\n",
      "Iteration 92: Normalized Reconstruction Error = 1.727603\n",
      "Iteration 93: Normalized Reconstruction Error = 1.727666\n",
      "Iteration 94: Normalized Reconstruction Error = 1.727731\n",
      "Iteration 95: Normalized Reconstruction Error = 1.727798\n",
      "Iteration 96: Normalized Reconstruction Error = 1.727867\n",
      "Iteration 97: Normalized Reconstruction Error = 1.727940\n",
      "Iteration 98: Normalized Reconstruction Error = 1.728015\n",
      "Iteration 99: Normalized Reconstruction Error = 1.728095\n",
      "Iteration 100: Normalized Reconstruction Error = 1.728177\n",
      "Final X shape: (100, 5)\n",
      "Final Y shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "Xk, Yk = nmf_with_compression(A, r, rOV, w)\n",
    "print(\"Final X shape:\", Xk.shape)\n",
    "print(\"Final Y shape:\", Yk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with Structured Compression (MU SkLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_mu(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.162617\n",
      "Final W shape: (100, 45)\n",
      "Final H shape: (45, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 45  # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_mu(A, r, rOV, w,1200,1e-5)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Reconstruction Error: 0.5412531442585214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tom\\Desktop\\python\\stat-6104\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition._nmf import _initialize_nmf\n",
    "\n",
    "def randomized_compression_srht(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Compute a low-rank approximation Q of A using SRHT.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure n is a power of 2 by padding A with zeros if necessary\n",
    "    n_padded = 2 ** int(np.ceil(np.log2(n)))  # Smallest power of 2 >= n\n",
    "    if n_padded != n:\n",
    "        A_padded = np.zeros((m, n_padded))\n",
    "        A_padded[:, :n] = A  # Pad with zeros\n",
    "        A = A_padded\n",
    "        n = n_padded\n",
    "\n",
    "    # Step 1: Generate SRHT matrix\n",
    "    H = hadamard(n)  # Hadamard matrix of size n x n\n",
    "    D = np.diag(np.random.choice([-1, 1], size=n))  # Random diagonal matrix\n",
    "    S = np.random.choice(n, size=d, replace=False)  # Random subsampling matrix\n",
    "    Omega_L = (H @ D)[:, S] / np.sqrt(d)  # SRHT matrix (n x d)\n",
    "\n",
    "    # Step 2: Compute B = (A A^T)^w A Omega_L\n",
    "    B = A @ Omega_L  # Initial multiplication: A Omega_L\n",
    "    for _ in range(w):\n",
    "        B = (A @ A.T) @ B  # Power iteration: (A A^T) B\n",
    "\n",
    "    # Step 3: Compute the orthogonal basis Q using QR decomposition\n",
    "    Q, _ = np.linalg.qr(B)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def srht_nndsvd_initialization(A, r, rOV, w):\n",
    "    \"\"\"\n",
    "    Combine SRHT and NNDSVD for NMF initialization.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Compute low-rank approximation using SRHT\n",
    "    Q = randomized_compression_srht(A, r, rOV, w)\n",
    "\n",
    "    # Step 2: Project Q onto the non-negative orthant\n",
    "    Q_non_neg = np.maximum(Q, 0)\n",
    "\n",
    "    # Step 3: Apply NNDSVD to Q_non_neg to initialize W and H\n",
    "    # Use the first r columns of Q_non_neg for initialization\n",
    "    W_init = Q_non_neg[:, :r]\n",
    "\n",
    "    # Initialize H using NNDSVD on the low-rank approximation\n",
    "    _, H_init = _initialize_nmf(A, r, init='nndsvd')\n",
    "\n",
    "    # Ensure W_init and H_init are C-contiguous\n",
    "    W_init = np.ascontiguousarray(W_init)\n",
    "    H_init = np.ascontiguousarray(H_init)\n",
    "\n",
    "    return W_init, H_init\n",
    "\n",
    "# Example usage\n",
    "m, n = 1000, 500  # Dimensions of A\n",
    "r = 50  # Target rank\n",
    "rOV = 10  # Oversampling parameter\n",
    "w = 2  # Power iteration exponent\n",
    "\n",
    "# Generate a random non-negative matrix A\n",
    "A = np.abs(np.random.randn(m, n))  # Ensure non-negativity\n",
    "\n",
    "# Initialize W and H using SRHT + NNDSVD\n",
    "W, H = srht_nndsvd_initialization(A, r, rOV, w)\n",
    "\n",
    "# Run NMF\n",
    "model = NMF(n_components=r, init='custom', random_state=0)\n",
    "W_final = model.fit_transform(A, W=W, H=H)\n",
    "H_final = model.components_\n",
    "\n",
    "# Evaluate reconstruction error\n",
    "reconstruction_error = np.linalg.norm(A - W_final @ H_final, ord='fro') / np.linalg.norm(A, ord='fro')\n",
    "print(\"Normalized Reconstruction Error:\", reconstruction_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF Structured Compression (Coord Desc. SKLearn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def structured_compression_cd(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using custom initialization with compression matrices L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R\n",
    "    L = randomized_compression(A, r, rOV, w)  # L ∈ R^(m x d)\n",
    "    R = randomized_compression(A.T, r, rOV, w).T  # R ∈ R^(d x n)\n",
    "\n",
    "    # Step 2: Custom initialization for W and H using projections\n",
    "    W = A @ R.T  # W ∈ R^(m x d)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='cd', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Reconstruction Error = 0.535564\n",
      "Final W shape: (100, 5)\n",
      "Final H shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = structured_compression_cd(A, r, rOV, w,500)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n",
      "(10, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m rOV \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m# Oversampling parameter\u001b[39;00m\n\u001b[1;32m     95\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m# Exponent\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m W, H \u001b[38;5;241m=\u001b[39m \u001b[43mnmf_with_fjlt_initialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrOV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal W shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal H shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, H\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[24], line 66\u001b[0m, in \u001b[0;36mnmf_with_fjlt_initialization\u001b[0;34m(A, r, rOV, w, max_iter, tol)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(R\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Step 2: Custom initialization for W and H using L and R\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m  \u001b[38;5;66;03m# W ∈ R^(m x d) = (100, 10)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m W \u001b[38;5;241m=\u001b[39m W[:, :r]  \u001b[38;5;66;03m# Take the first r columns for W ∈ R^(m x r) = (100, 5)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m H \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m A  \u001b[38;5;66;03m# H ∈ R^(d x n) = (10, 50)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 50)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def fast_jlt_transform(A, target_dim):\n",
    "    \"\"\"\n",
    "    Apply the Fast Johnson-Lindenstrauss Transform (FJLT) to matrix A.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    target_dim (int): Target reduced dimension\n",
    "\n",
    "    Returns:\n",
    "    A_reduced (numpy array): Dimension-reduced matrix (target_dim x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Step 1: Generate a Hadamard matrix (nearest power of 2)\n",
    "    H_dim = 2**int(np.ceil(np.log2(m)))  # Get the nearest power of 2\n",
    "    H = hadamard(H_dim)[:m, :m]  # Truncate to match dimensions\n",
    "\n",
    "    # Step 2: Create a diagonal sign matrix D (random ±1)\n",
    "    D = np.diag(np.random.choice([-1, 1], size=m))\n",
    "\n",
    "    # Step 3: Apply Hadamard and sign-flipping\n",
    "    HD = H @ D @ A  # (m x n)\n",
    "\n",
    "    # Step 4: Generate a sparse projection matrix P (proper scaling)\n",
    "    P = sparse_random(target_dim, m, density=1/target_dim, format='csr', random_state=42).toarray()\n",
    "    P *= np.sqrt(1 / target_dim)  # Scale projection\n",
    "\n",
    "    # Step 5: Compute the reduced matrix\n",
    "    A_reduced = P @ HD  # (target_dim x n)\n",
    "\n",
    "    return A_reduced\n",
    "\n",
    "def nmf_with_fjlt_initialization(A, r, rOV, w, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Compute NMF using FJLT for initialization of L and R.\n",
    "\n",
    "    Parameters:\n",
    "    A (numpy array): Input matrix (m x n)\n",
    "    r (int): Target rank\n",
    "    rOV (int): Oversampling parameter\n",
    "    w (int): Exponent for the power iteration\n",
    "    max_iter (int): Maximum number of iterations\n",
    "    tol (float): Convergence tolerance\n",
    "\n",
    "    Returns:\n",
    "    W (numpy array): Nonnegative matrix (m x r)\n",
    "    H (numpy array): Nonnegative matrix (r x n)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    d = r + rOV  # Effective reduced dimension\n",
    "\n",
    "    # Ensure A is nonnegative\n",
    "    A = np.abs(A)\n",
    "\n",
    "    # Step 1: Compute compression matrices L and R using FJLT\n",
    "    L = fast_jlt_transform(A, d)  # L ∈ R^(d x m) = (10, 100)\n",
    "    print(L.shape)\n",
    "    R = fast_jlt_transform(A.T, d)  # R ∈ R^(d x n) = (10, 50)\n",
    "    print(R.shape)\n",
    "    # Step 2: Custom initialization for W and H using L and R\n",
    "    W = A @ R.T  # W ∈ R^(m x d) = (100, 10)\n",
    "    W = W[:, :r]  # Take the first r columns for W ∈ R^(m x r) = (100, 5)\n",
    "\n",
    "    H = L.T @ A  # H ∈ R^(d x n) = (10, 50)\n",
    "    H = H[:r, :]  # Take the first r rows for H ∈ R^(r x n) = (5, 50)\n",
    "\n",
    "    # Ensure W and H are nonnegative\n",
    "    W = np.abs(W)\n",
    "    H = np.abs(H)\n",
    "\n",
    "    # Step 3: Use scikit-learn's NMF solver for updates\n",
    "    nmf = NMF(n_components=r, init='custom', solver='mu', max_iter=max_iter, tol=tol, random_state=42)\n",
    "\n",
    "    # Fit the model using custom initialization\n",
    "    W = nmf.fit_transform(A, W=W, H=H)  # Update W and H using A\n",
    "    H = nmf.components_\n",
    "\n",
    "    # Step 4: Compute reconstruction error\n",
    "    reconstruction_error = np.linalg.norm(A - W @ H, 'fro')\n",
    "    normalized_error = reconstruction_error / np.linalg.norm(A, 'fro')\n",
    "    print(f\"Final Normalized Reconstruction Error = {normalized_error:.6f}\")\n",
    "\n",
    "    return W, H\n",
    "\n",
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "A = np.abs(np.random.randn(100, 50))  # Nonnegative matrix (100 x 50)\n",
    "r = 5   # Target rank\n",
    "rOV = 5 # Oversampling parameter\n",
    "w = 2   # Exponent\n",
    "\n",
    "W, H = nmf_with_fjlt_initialization(A, r, rOV, w)\n",
    "print(\"Final W shape:\", W.shape)\n",
    "print(\"Final H shape:\", H.shape)\n",
    "print(\"W (nonnegative):\\n\", W)\n",
    "print(\"H (nonnegative):\\n\", H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
